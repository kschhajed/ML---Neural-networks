{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhedcb2qdr41"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2rjFlAzYPyF"
      },
      "source": [
        "Preprocessing and scaling data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3qdAdthnS9I"
      },
      "source": [
        "#Function to scale data to avoid large values in e^(-z) of sigmoid function, returns scaled data\n",
        "def scaling(dataframe):\n",
        "  scaleddata = (dataframe - dataframe.mean()) / (dataframe.max() - dataframe.min())\n",
        "  return scaleddata\n",
        " \n",
        "#Reading data and shuffling\n",
        "df = pd.read_csv(r\"diabetes.csv\")\n",
        "\n",
        "#Extracting original outputs in ycomplete\n",
        "ycomplete = df[\"Outcome\"].values.reshape(df[\"Outcome\"].shape[0],1)\n",
        "\n",
        "#Scaling and converting to numpy array\n",
        "data_scaled = scaling(df.iloc[:,:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1A13HUjmBcA"
      },
      "source": [
        "Part 1 - Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM1AoEZUmHxt"
      },
      "source": [
        "data_scaled_cpy = data_scaled.copy()\n",
        "data_scaled_cpy.insert(data_scaled_cpy.shape[1],'Bias',1)\n",
        "\n",
        "#Function to find z and sigmoid function using provided data\n",
        "def hypothesis(w, traindata):\n",
        "    z = np.dot(traindata,w)\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "#Function to check output on validation or test dataset\n",
        "def validatingtesting(w,data,y_in):\n",
        "    class_rep = np.where( hypothesis(w, data) > 0.5 , 1, 0)\n",
        "    accuracy = (1- (np.average((class_rep - y_in)**2))) * 100\n",
        "    return accuracy\n",
        "\n",
        "#Method to calculate cost function\n",
        "def costcal(y,h):\n",
        "    cost = np.sum(- (y * np.log(h)) - ((1-y) * np.log(1 - h)))/len(h)\n",
        "    return cost\n",
        "\n",
        "def main():\n",
        "\n",
        "    #Splitting data into 60% training, 20% validation and 20% testing sets\n",
        "\n",
        "    traindata, validatedata, y_train, y_validate= train_test_split(data_scaled_cpy.values,ycomplete,test_size=0.4,random_state=2)\n",
        "    validatedata, testdata, y_validate, y_test= train_test_split(validatedata, y_validate,test_size=0.5,random_state=2)\n",
        "   \n",
        "    #Initializing weights\n",
        "    w = np.zeros(9).reshape(9,1)\n",
        "\n",
        "    #Hyperparameters\n",
        "    alpha = 0.03\n",
        "    epochs = 200\n",
        "\n",
        "    h = hypothesis(w, traindata)\n",
        "    valacc=[]\n",
        "    trainacc=[]\n",
        "    testacc=[]\n",
        "    cost_train=[]\n",
        "    cost_val=[]\n",
        "    cost_test=[]\n",
        "\n",
        "    for i in range(epochs):\n",
        "        w = w - alpha * (((h-y_train).T).dot(traindata)).T\n",
        "        h = hypothesis(w, traindata)\n",
        "\n",
        "        cost_train.append(costcal(y_train,h))\n",
        "        cost_val.append(costcal(y_validate,hypothesis(w, validatedata)))\n",
        "        cost_test.append(costcal(y_test,hypothesis(w, testdata)))\n",
        "\n",
        "        trainacc.append(validatingtesting(w,traindata,y_train))\n",
        "        valacc.append(validatingtesting(w,validatedata,y_validate))\n",
        "        testacc.append(validatingtesting(w,testdata,y_test))\n",
        "\n",
        "    print(\"Weights after training: \", w.T)\n",
        "\n",
        "    \n",
        "    plt.figure(\"Accuracy vs Epochs\")\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(valacc, 'r', label = 'Validation')\n",
        "    plt.plot(trainacc, 'g', label = 'Training')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc = 'upper left')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "    plt.figure(\"Cost function vs Epochs\")\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(cost_train, 'r', label = 'Training')\n",
        "    plt.plot(cost_val, 'g', label = 'Validation')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Cost function\")\n",
        "    plt.legend(loc = 'upper left')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Validation dataset accuracy:\",valacc[len(valacc) - 1])\n",
        "\n",
        "    print(\"Training dataset accuracy:\",trainacc[len(trainacc) -1])\n",
        "\n",
        "    print(\"Testing dataset accuracy:\", testacc[len(testacc) -1])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU8wRo-dp7O4"
      },
      "source": [
        "##Part 2- Neural Network and\n",
        "##Part 3- Regularizations and comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoACaE-q1ufs"
      },
      "source": [
        "\n",
        "#Defining training, validation and testing datasets\n",
        "x_train, x_exp, y_train, y_exp= train_test_split(data_scaled.values,ycomplete,test_size=0.4,random_state=2)\n",
        "x_val, x_test, y_val, y_test= train_test_split(x_exp, y_exp,test_size=0.5,random_state=2)\n",
        "\n",
        "# Creating the model\n",
        "\n",
        "model = Sequential()\n",
        "#Please change flag manually to view results\n",
        "flag = 4 \n",
        "\n",
        "if flag == 1:\n",
        "  model.add(Dense(25, activation='relu', input_dim= x_train.shape[1]))  #Input layer\n",
        "  model.add(Dense(15, activation='relu')) # Hidden layer.\n",
        "  model.add(Dense(10, activation='relu')) # Hidden layer.\n",
        "  model.add(Dense(1, activation='sigmoid'))  # Output layer.(Since there is 1 class)\n",
        "\n",
        "if flag == 2:\n",
        "  model.add(Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.01), input_dim= x_train.shape[1]))  #Input layer\n",
        "  model.add(Dense(15, activation='relu',kernel_regularizer=tf.keras.regularizers.l1(0.01))) # Hidden layer.\n",
        "  model.add(Dense(10, activation='relu',kernel_regularizer=tf.keras.regularizers.l1(0.01))) # Hidden layer.\n",
        "  model.add(Dense(1, activation='sigmoid'))  # Output layer.(Since there is 1 class)\n",
        "\n",
        "if flag == 3:\n",
        "  model.add(Dense(25, activation='relu', kernel_regularizer= regularizers.l2(0.01), input_dim= x_train.shape[1]))  #Input layer\n",
        "  model.add(Dense(15, activation='relu',kernel_regularizer= regularizers.l2(0.01))) # Hidden layer.\n",
        "  model.add(Dense(10, activation='relu',kernel_regularizer= regularizers.l2(0.01))) # Hidden layer.\n",
        "  model.add(Dense(1, activation='sigmoid'))  # Output layer.(Since there is 1 class)\n",
        "\n",
        "if flag == 4:\n",
        "  model.add(Dense(25, activation='relu', input_dim= x_train.shape[1])) #Input layer\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(15, activation='relu')) # Hidden layer.\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(10, activation='relu')) # Hidden layer.\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))  # Output layer.(Since there is 1 class)\n",
        "\n",
        "\n",
        "epoch = 250\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=0.08,decay=0.008), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model.\n",
        "history = model.fit(x_train,y_train,validation_data=(x_val, y_val), epochs = epoch, batch_size = 16, verbose=0)\n",
        "\n",
        "# Evaluating the model\n",
        "\n",
        "loss_train, accuracy_train = model.evaluate(x_train, y_train, verbose=0)\n",
        "print('\\n Train data Loss:', loss_train, '\\tTrain data Accuracy:', accuracy_train)\n",
        "loss_valid, accuracy_valid = model.evaluate(x_val, y_val,verbose=0)\n",
        "print('\\n Validation data Loss:', loss_valid, '\\tValidation data Accuracy:', accuracy_valid)\n",
        "loss_test, accuracy_test = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('\\n Test data Loss:', loss_test, '\\tTest data Accuracy:', accuracy_test)\n",
        "\n",
        "# Plot the accuracy and loss.\n",
        "\n",
        "plt.figure(\"Accuracy vs Epochs\")\n",
        "plt.plot(history.history['accuracy'], 'r', label = 'Training')\n",
        "plt.plot(history.history['val_accuracy'], 'g', label = 'Validation')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(\"Loss vs Epochs\")\n",
        "plt.plot(history.history['loss'], 'r', label = 'Training')\n",
        "plt.plot(history.history['val_loss'], 'g', label = 'Validation')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cost function\")\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}